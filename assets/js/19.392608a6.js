(window.webpackJsonp=window.webpackJsonp||[]).push([[19],{584:function(t,s,a){t.exports=a.p+"assets/img/多分类回归模型.01a796c3.png"},585:function(t,s,a){t.exports=a.p+"assets/img/理解soft.f7dac758.png"},586:function(t,s,a){t.exports=a.p+"assets/img/直观理解softmax做了什么.beb73e06.png"},587:function(t,s,a){t.exports=a.p+"assets/img/softmax的损失函数定义.de712ce3.png"},588:function(t,s,a){t.exports=a.p+"assets/img/softmax的梯度下降.b88037b3.png"},749:function(t,s,a){"use strict";a.r(s);var r=a(26),e=Object(r.a)({},(function(){var t=this,s=t.$createElement,r=t._self._c||s;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"多结果分类器-softmax-回归"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#多结果分类器-softmax-回归"}},[t._v("#")]),t._v(" 多结果分类器：Softmax 回归")]),t._v(" "),r("h2",{attrs:{id:"_1-多结果情形"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-多结果情形"}},[t._v("#")]),t._v(" 1. 多结果情形")]),t._v(" "),r("p",[t._v("Logitic 回归输出结果为 0 / 1，当结果有多个可能性时，就不适用了。")]),t._v(" "),r("p",[r("img",{attrs:{src:a(584),alt:""}})]),t._v(" "),r("h2",{attrs:{id:"_1-1-softmax-回归"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-softmax-回归"}},[t._v("#")]),t._v(" 1.1 Softmax 回归")]),t._v(" "),r("p",[t._v("假设，训练得到了四个数据，需要通过这四个数值映射到分类结果，一种思路就是将最大值置为 1，其它都看作 0。这种映射方式太生硬。")]),t._v(" "),r("p",[t._v("相对的，Softmax 方法本质上就是使这个过程更加温和（Soft）。")]),t._v(" "),r("p",[r("img",{attrs:{src:a(585),alt:""}})]),t._v(" "),r("h3",{attrs:{id:"_1-1-直观理解-softmax-做了什么"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-直观理解-softmax-做了什么"}},[t._v("#")]),t._v(" 1.1 直观理解 Softmax 做了什么")]),t._v(" "),r("p",[r("img",{attrs:{src:a(586),alt:""}})]),t._v(" "),r("h2",{attrs:{id:"_2-softmax-的训练"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2-softmax-的训练"}},[t._v("#")]),t._v(" 2. Softmax 的训练")]),t._v(" "),r("h3",{attrs:{id:"_2-1-损失函数定义"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-损失函数定义"}},[t._v("#")]),t._v(" 2.1 损失函数定义")]),t._v(" "),r("p",[r("img",{attrs:{src:a(587),alt:""}})]),t._v(" "),r("h3",{attrs:{id:"_2-2-梯度下降"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-梯度下降"}},[t._v("#")]),t._v(" 2.2 梯度下降")]),t._v(" "),r("p",[t._v("含有 softmax 层的模型与前述模型相比，最大的不同点是 "),r("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[r("mjx-math",{staticClass:" MJX-TEX"},[r("mjx-mi",{staticClass:"mjx-i"},[r("mjx-c",{attrs:{c:"d"}})],1),r("mjx-msup",[r("mjx-mi",{staticClass:"mjx-i"},[r("mjx-c",{attrs:{c:"Z"}})],1),r("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[r("mjx-TeXAtom",{attrs:{size:"s"}},[r("mjx-mo",{staticClass:"mjx-n"},[r("mjx-c",{attrs:{c:"["}})],1),r("mjx-mi",{staticClass:"mjx-i"},[r("mjx-c",{attrs:{c:"L"}})],1),r("mjx-mo",{staticClass:"mjx-n"},[r("mjx-c",{attrs:{c:"]"}})],1)],1)],1)],1)],1)],1),t._v(" 的计算，其公式如下：")],1),t._v(" "),r("p",[r("img",{attrs:{src:a(588),alt:""}})]),t._v(" "),r("h3",{attrs:{id:"_2-3-后向传播"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-后向传播"}},[t._v("#")]),t._v(" 2.3 后向传播")])])}),[],!1,null,null,null);s.default=e.exports}}]);