# 优化算法

优化算法能够让模型训练得更快。

## 1. Mini-batch 梯度下降法

向量化使得模型计算的速度更快，但是如果样本量特别大的时候，执行一次学习迭代需要计算所有样本一次，代价仍然太大。

Mini-batch 的思想就是在训练整个数据集之前，先使用部分数据集让梯度下降一些，以提高训练速度。

### 1.1 切分数据集

![](./imgs/将数据集切分为minibatch.png)

### 1.2 训练过程

![](./imgs/minibatch的训练过程.png)

### 1.3 对 Mini-batch 算法的理解

在使用 Mini-batch 进行训练时，相邻的两步梯度下降都是在不同的数据集上进行的，所以它的下降曲线并不是平滑的：

![](./imgs/minibatch的成本下降曲线.png)


### 1.4 Mini-batch 的分包大小选择问题

一个 batch 中包含的样本数量 m 是一个超参，它对训练的影响如下：

![](./imgs/minibatch的大小选择.png)

当 m 等于整个数据集的大小时，它实际上就等同于普通的 Batch 算法，还是会面临迭代一次就要计算整个数据集一次的成本问题。

当 m 等于 1 时，它是随机梯度下降法，随机梯度下降法永远不会收敛到最小值，而是在最小值附近波动。它失去了向量化带来的多个样本计算加速优势。

所以，m 的取值通常介于 1 和 max 之间，既能够加快梯度下降速度，又有向量化的加成。

## 2. 指数加权平均

指数加权平均是使用其他优化算法的基础。

### 2.1 指数加权平均法的原理

关键公式：

$$v_t = \beta v_{t - 1} + (1 - \beta)\theta _t$$

![](./imgs/指数加权平均法的原理.png)

### 2.2 理解指数加权平均法

指数加权平均法的关键参数是 $\beta$，它的取值决定了当前数据会受到之前多久数据的影响。

实现：

![](./imgs/指数加权平均法的代码实现.png)

其与一般平均法的优势在于：

1. 代码量小，只需要一行代码
2. 内存占用小：它只需要存储前一天的数据即可

### 2.3 指数加权平均法的偏差修正

因为指数加权平均法在实现时，首先把第零天初始化为 0，所以前几天的数据会出现偏差：本应是绿线，但实际为紫线：

![](./imgs/指数加权平均法的偏差修正.png)

通过加入偏差修正，能够矫正初期的数据偏差，但是在深度学习中，很多人并不在意这个修正。
## 3. 动量梯度下降法

## 4. RMSprop

## 5. Adam 优化算法

## 6. 关于学习率衰减

## 7. 局部最优的问题
